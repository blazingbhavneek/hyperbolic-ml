{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blazingbhavneek/hyperbolic-ml/blob/main/L_Hydra_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd2cokU_NY_a",
        "outputId": "b3efb4a3-8cde-4058-b1f1-25198b55a3d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_file = '/content/L-hydra.zip'\n",
        "target_dir = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_dir)\n"
      ],
      "metadata": {
        "id": "Uxr-s61AE9ck"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPJvO2IgnOtS",
        "outputId": "82bc5329-daa2-422a-c6fa-ffd27c7f643c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp310-cp310-linux_x86_64.whl size=3365643 sha256=8b6158d8e895c3ce504d80551881d3d560c8ba23d0e9878b0348016db2b78232\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/1b/b5/97ec4cfccdde26e0f3590ad6e09a5242d508dff09704ef86c1\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.4\n",
            "mkdir: cannot create directory ‘/content/L-hydra/data/amazon/temp’: File exists\n",
            "/content/L-hydra\n"
          ]
        }
      ],
      "source": [
        "# !python -m pip install snap-stanford\n",
        "!pip install mpi4py\n",
        "!mkdir /content/L-hydra/data/amazon/temp\n",
        "%cd /content/L-hydra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NSNupMjqoWQw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys, time, pdb, os, pickle\n",
        "from numpy import *\n",
        "import networkx as nx\n",
        "\n",
        "###########################################################################\n",
        "def randX(N,land, d):\n",
        "  n = N-land\n",
        "  n_temp = n\n",
        "  param = 0.4  #distance thresold\n",
        "  g = nx.generators.random_geometric_graph(n_temp, param, seed = 1)\n",
        "  W = np.zeros((n_temp, n_temp))\n",
        "  for (x,y) in g.edges:\n",
        "    W[x][y] = np.random.uniform(0.1,3)\n",
        "  W_hat = W + W.T\n",
        "  L = np.diag(np.matmul(W_hat,np.ones(W_hat.shape[0]))) - W_hat\n",
        "  print(\"L\",L)\n",
        "  np.random.seed(1)\n",
        "  X_na = np.random.multivariate_normal(np.zeros(n), np.linalg.pinv(L),d+1)\n",
        "\n",
        "  print(\"Size of original features\", X_na.shape)\n",
        "\n",
        "  np.random.seed(5)\n",
        "  X_a = np.random.normal(0,1,size=(d+1,land))\n",
        "\n",
        "  X = np.hstack((X_a, X_na))\n",
        "  print(\"Xa shape\",X_a.shape)\n",
        "\n",
        "  for i in range(N):\n",
        "    X[0,i] = np.sqrt(1+np.linalg.norm(X[1:d+1,i])**2);\n",
        "  \n",
        "  print(\"X shape\",X.shape)\n",
        "  print(\"X\",X)\n",
        "  return X\n",
        "\n",
        "def x2hdm(N,land, X,d):\n",
        "\n",
        "  n = N-land\n",
        "  G = x2hgram(N,d,X)\n",
        "  D = _arccosh(G)\n",
        "  print(\"dcom shape\", D)\n",
        "\n",
        "  return D \n",
        "\n",
        "def _arccosh(G):\n",
        "  D = np.arccosh(-G)\n",
        "  return D\n",
        "\n",
        "def x2hgram(N,d, X):\n",
        "  n = N-d\n",
        "  #######################################\n",
        "  # print(\"shape of you\", X.shape)\n",
        "  X_ = X\n",
        "  for n in range(N):\n",
        "      x = X_[:,n]\n",
        "      X_[:,n] = projectX(N,d,x)\n",
        "  # print(\"shape of you after\", X_.shape)\n",
        "  #######################################\n",
        "  G = x2lgram(N,d, X_)\n",
        "  # print(\"valid G\", G)\n",
        "  #######################################\n",
        "  E1 = G-valid(G) \n",
        "  # print('E1=',E1)\n",
        "  if np.linalg.norm(E1,'fro') > 1e-10:\n",
        "      print(np.linalg.norm(E1,'fro'))\n",
        "      print('inaccuracy in  x2hgram - ')\n",
        "  #######################################\n",
        "  E2 = G-l_rankPrj(G)\n",
        "  # print('E2=',E2)\n",
        "  if np.linalg.norm(E2,'fro') > 1e-10:\n",
        "      print(np.linalg.norm(E2,'fro'))\n",
        "      print('inaccuracy in  x2hgram -- ')\n",
        "  return G\n",
        "\n",
        "def projectX(N,d,x):\n",
        "  n= N-d\n",
        "  \n",
        "  #######################################\n",
        "  H = np.eye(d+1)\n",
        "  H[0,0] = -1\n",
        "  I = np.eye(d+1)\n",
        "  #######################################\n",
        "  x0 = x[0]\n",
        "  #print('x0 is:', x[0])\n",
        "  eps = 1e-15\n",
        "  center = 0\n",
        "  error = abs(h_norm(x,N,d)+1)\n",
        "  #print(error)\n",
        "  A_opt = I\n",
        "  for i in range(50):\n",
        "      l = 10**(-i)\n",
        "      if x0 > 0:\n",
        "          lambda_min = max(center-l, -1+eps)\n",
        "          lambda_max = min(center+l, 1-eps)\n",
        "          number = 50\n",
        "      else:\n",
        "          lambda_min = max(center-l*10000, 1+eps)\n",
        "          lambda_max = center+l*10000\n",
        "          number = 100\n",
        "      lambda_list = np.linspace(lambda_min,lambda_max,num=number)\n",
        "      for lambda_ in lambda_list:\n",
        "          A = np.linalg.inv(I + lambda_*H)\n",
        "          x_l = np.matmul(A,x)\n",
        "          if abs(h_norm(x_l,N,d)+1) < error:\n",
        "              A_opt = A\n",
        "              error = abs(h_norm(x_l,N,d)+1)\n",
        "              center = lambda_\n",
        "  x_opt = np.matmul(A_opt,x)\n",
        "  if error > 1e-5:\n",
        "      print('hi:',error)\n",
        "      print(center)\n",
        "  # print(\"shape of x_opt\",x_opt.shape)    \n",
        "  return x_opt\n",
        "\n",
        "def x2lgram(N,d,X):\n",
        "  n= N-d\n",
        "  #######################################\n",
        "  H = np.eye(d+1)\n",
        "  H[0,0] = -1\n",
        "  #######################################\n",
        "  G = np.matmul(np.matmul(X.T,H),X)\n",
        "  return G\n",
        "\n",
        "def h_norm(x,N,d):\n",
        "  n= N-d\n",
        "  #######################################\n",
        "  H = np.eye(d+1)\n",
        "  H[0,0] = -1\n",
        "  #######################################\n",
        "  x_norm = np.matmul(np.matmul(x.T,H),x)\n",
        "  return x_norm\n",
        "\n",
        "def valid(G):\n",
        "  np.fill_diagonal(G, -1)\n",
        "  G[G >= -1] = -1\n",
        "  \n",
        "  return G\n",
        "\n",
        "def l_rankPrj(G):\n",
        "  X = lgram2x(N,d,G)\n",
        "  \n",
        "  return x2lgram(N,d,X)\n",
        "\n",
        "def lgram2x(N,d,G):\n",
        "  #######################################\n",
        "  n= N-d\n",
        "  #######################################\n",
        "  w, v = np.linalg.eig(G)\n",
        "  w = w.real\n",
        "  v = v.real\n",
        "  # print(\"d\",d)\n",
        "  # print(\"len w\", len(w))\n",
        "  lambda_0 = np.amin(w)\n",
        "  ind_0 = np.argmin(w)\n",
        "  w = np.delete(w, ind_0)\n",
        "  ind = np.argsort(-w)\n",
        "  w = -np.sort(-w)\n",
        "  ind = ind[:d]\n",
        "  w = w[:d]\n",
        "  #######################################\n",
        "  lambda_ = np.concatenate((abs(lambda_0), w), axis=None) \n",
        "  lambda_[lambda_ <= 0] = 0\n",
        "  lambda_ = np.sqrt(lambda_)\n",
        "\n",
        "  ind_ = np.concatenate((ind_0, ind+1), axis=None) \n",
        "  v = v[:, ind_]\n",
        "  X = np.matmul(np.diag(lambda_),v.T)\n",
        "  #######################################\n",
        "  if X[0,0] < 0:\n",
        "      X = -X\n",
        "  #######################################\n",
        "  return X                    \n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YTjQWSO6nVFh"
      },
      "outputs": [],
      "source": [
        "# Parameters for Preprocessing\n",
        "name = \"amazon\"\n",
        "snapdir = \"/content/L-hydra/data/amazon/snap/\"\n",
        "savedir = \"/content/L-hydra/data/amazon/\"\n",
        "nprocs = 4\n",
        "nlandmarks = 101\n",
        "nNodes = 250\n",
        "nvalidation = 40\n",
        "maxprocs4seed = 8\n",
        "split = True\n",
        "nodes_per_run = 40      #Check splits and procs are similar\n",
        "# Parameters for L-hydra / L-hydra+\n",
        "curvature = 1\n",
        "dim = 100\n",
        "alpha = 1\n",
        "equi_adj = 0.5\n",
        "# Parameters for Parallelization of L-Hydra+\n",
        "nrepeat = 8\n",
        "graphsrc = 'amazon_edges' # original file from SNAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_XdcjsXJKXlG"
      },
      "outputs": [],
      "source": [
        "with open('data/amazon/nNodes.txt', 'w') as f:\n",
        "    f.write(str(nNodes))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F2ZEZVzOoNte"
      },
      "outputs": [],
      "source": [
        "location = snapdir # directory for input data (contains edge list from snap)\n",
        "dirloc = location\n",
        "datafile = dirloc + graphsrc\n",
        "newfilename = dirloc + graphsrc + '_remapped' # remapped edge list\n",
        "filename4edges = dirloc + graphsrc\n",
        "import numpy.random as random\n",
        "metadata = {\"nL\":nlandmarks, \"nVal\":nvalidation, \"datadir\":savedir}\n",
        "metadata[\"nnodes\"] = nNodes\n",
        "# Choose random seed for selecting landmarks and nonlandmark testing pairs\n",
        "rn = random.RandomState(233)\n",
        "\n",
        "filename_adjmat = name + '_edges_remapped'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXbMEEARoN8j",
        "outputId": "93096ee3-d78e-4576-a0c5-2b13e17dc6b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L [[ 61.1149311    0.           0.         ...  -2.86774057   0.\n",
            "    0.        ]\n",
            " [  0.         101.7554889   -1.77854823 ...   0.          -2.06482899\n",
            "    0.        ]\n",
            " [  0.          -1.77854823  99.96648083 ...  -2.17891179   0.\n",
            "   -2.95650388]\n",
            " ...\n",
            " [ -2.86774057   0.          -2.17891179 ...  97.51578173   0.\n",
            "    0.        ]\n",
            " [  0.          -2.06482899   0.         ...   0.          57.80682808\n",
            "   -0.61368551]\n",
            " [  0.           0.          -2.95650388 ...   0.          -0.61368551\n",
            "   95.01123299]]\n",
            "Size of original features (100, 149)\n",
            "Xa shape (100, 101)\n",
            "X shape (100, 250)\n",
            "X [[ 9.49370129e+00  9.01923884e+00  9.94081131e+00 ...  1.45132759e+00\n",
            "   1.71011232e+00  1.42311736e+00]\n",
            " [ 1.70764452e-01 -1.75374086e+00  6.30296477e-01 ... -4.75269770e-02\n",
            "  -1.12837353e-02  7.28999620e-03]\n",
            " [-6.10516401e-01 -1.70388541e+00  1.94437378e-01 ... -8.14066201e-02\n",
            "   7.46976018e-02  7.10660972e-02]\n",
            " ...\n",
            " [-2.83795276e-01  2.45890322e-01 -4.24178925e-02 ...  3.15892278e-02\n",
            "  -2.01152700e-01  7.93619733e-03]\n",
            " [-1.28087708e+00  1.01399277e+00 -2.09408169e-01 ... -1.47153853e-01\n",
            "   3.53389134e-02  1.71239971e-01]\n",
            " [ 6.11227836e-02 -1.17766987e+00 -1.47602874e+00 ... -5.06494607e-03\n",
            "   4.27015196e-02 -2.95164017e-02]]\n",
            "X (100, 250)\n",
            "dcom shape [[0.         5.05111803 5.27137132 ... 3.25532037 3.57069959 3.36272649]\n",
            " [5.05111803 0.         5.15607648 ... 3.32006952 3.30727602 3.28179209]\n",
            " [5.27137132 5.15607648 0.         ... 3.44903568 3.51688696 3.48254172]\n",
            " ...\n",
            " [3.25532037 3.32006952 3.44903568 ... 0.         1.66121298 1.39338312]\n",
            " [3.57069959 3.30727602 3.51688696 ... 1.66121298 0.         1.48234427]\n",
            " [3.36272649 3.28179209 3.48254172 ... 1.39338312 1.48234427 0.        ]]\n",
            "D [[0.         5.05111803 5.27137132 ... 3.25532037 3.57069959 3.36272649]\n",
            " [5.05111803 0.         5.15607648 ... 3.32006952 3.30727602 3.28179209]\n",
            " [5.27137132 5.15607648 0.         ... 3.44903568 3.51688696 3.48254172]\n",
            " ...\n",
            " [3.25532037 3.32006952 3.44903568 ... 0.         1.66121298 1.39338312]\n",
            " [3.57069959 3.30727602 3.51688696 ... 1.66121298 0.         1.48234427]\n",
            " [3.36272649 3.28179209 3.48254172 ... 1.39338312 1.48234427 0.        ]]\n",
            "L2n (101, 250)\n",
            "L2n [[0.         5.05111803 5.27137132 ... 3.25532037 3.57069959 3.36272649]\n",
            " [5.05111803 0.         5.15607648 ... 3.32006952 3.30727602 3.28179209]\n",
            " [5.27137132 5.15607648 0.         ... 3.44903568 3.51688696 3.48254172]\n",
            " ...\n",
            " [5.11705049 5.20871794 5.22177807 ... 3.25537118 3.65719386 3.26909136]\n",
            " [5.34780751 5.22192359 5.45260512 ... 3.45716004 3.51284289 3.43598567]\n",
            " [4.96823022 5.04908429 5.22934816 ... 3.19726845 3.49087366 3.11928483]]\n"
          ]
        }
      ],
      "source": [
        "N = nNodes\n",
        "land = nlandmarks\n",
        "n = N-land\n",
        "d = dim-1\n",
        "X = randX(nNodes,land, d)\n",
        "print(\"X\",X.shape)\n",
        "D = x2hdm(nNodes,land, X,d)\n",
        "print(\"D\", D)\n",
        "\n",
        "D = (D+D.T)/2\n",
        "L2n = D[:nlandmarks,:]\n",
        "print(\"L2n\", L2n.shape)\n",
        "print(\"L2n\", L2n)\n",
        "\n",
        "nNodes = nNodes\n",
        "metadata[\"nnodes\"] = nNodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaGoLw4otxYZ",
        "outputId": "740bd768-2532-4153-ed77-3b2f752c411f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "landmarks 101\n",
            "total nodes 250\n"
          ]
        }
      ],
      "source": [
        "print(\"landmarks\", nlandmarks)\n",
        "print(\"total nodes\",nNodes)\n",
        "# print(\"distance matrix\", )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OnlFbKdCr6iw"
      },
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "import sys, time, pdb, os, pickle\n",
        "\n",
        "from numpy import *\n",
        "from scipy.optimize import minimize, fmin_l_bfgs_b\n",
        "\n",
        "'''\n",
        "Functions used during embedding processes\n",
        "\n",
        "First section: \n",
        "\tfunctions used during pre and post processing\n",
        "Second section: \n",
        "\tobjective functions for stress minimization\n",
        "Third section: functions for landmark embedding\n",
        "''' \n",
        "\n",
        "# **********************************\n",
        "# Pre and post processing functions\n",
        "# **********************************\n",
        "\n",
        "def preprocdata(nodes, landmarks, nodes_per_run, dataloc, maxprocs4seed=1000, cleanup=False):\n",
        "\tprint(\"\\n\", \"*\"*60)\n",
        "\tprint(\"Pre-processing: splitting D mtx for n = %i and L = %i\" %(nodes, landmarks))\n",
        "\tprint(\"*\"*60)\n",
        "\n",
        "\tif cleanup: clean(dataloc)\n",
        "\t\n",
        "\t# calculate the number of partitions for the D matrix\n",
        "\tn_split = int(ceil((nodes-landmarks)/float(nodes_per_run)))\n",
        "\tsavetxt(dataloc + '/nonland_nsplit.txt', [n_split],fmt='%i')\n",
        "\n",
        "\tprint(\"Splitting nonlandmark matrix\")\n",
        "\tpreproc(nodes, landmarks, dataloc, n_split, maxprocs4seed)\n",
        "\n",
        "def landmark_preproc(nodes, landmarks, dim, dataloc):\n",
        "\tinput_var = [\" -n \",\" -L \",\" -d \",\" -s \"]\n",
        "\tinput_val = [nodes, landmarks, dim, dataloc]\n",
        "\tinput_tot = \"\"\n",
        "\tfor i,var in enumerate(input_var):\n",
        "\t\tinput_tot += var + str(input_val[i])\n",
        "\treturn input_tot\n",
        "\n",
        "def nonlandmark_preproc(nodes, landmarks, dim, split, dataloc):\n",
        "\tinput_var = [\" -n \",\" -L \",\" -d \",\" -sp \", \" -s \"]\n",
        "\tinput_val = [nodes, landmarks, dim, split, dataloc]\n",
        "\tinput_tot = \"\"\n",
        "\tos.system('mkdir ' + os.path.join(dataloc, '/temp'))\n",
        "\tfor i,var in enumerate(input_var):\n",
        "\t\tinput_tot += var + str(input_val[i])\n",
        "\treturn input_tot\n",
        "\n",
        "def validation_preproc(nodes, landmarks, dim, nproc, dataloc, savedir):\n",
        "\tinput_var = [\" -n \",\" -L \",\" -d \",\" -p \", \" -s1 \", \" -s2 \"]\n",
        "\tinput_val = [nodes, landmarks, dim, nproc, dataloc, savedir]\n",
        "\tinput_tot = \"\"\n",
        "\tfor i,var in enumerate(input_var):\n",
        "\t\tinput_tot += var + str(input_val[i])\n",
        "\treturn input_tot\n",
        "\n",
        "def save_obj(obj, name, savedir=''):\n",
        "    with open(savedir + '/' + name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "def load_obj(name, savedir='' ):\n",
        "    with open(savedir + '/' + name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def clean(dataloc):\n",
        "\tprint(\"Cleaning up data folder...\")\n",
        "\tos.system('rm ' + os.path.join(dataloc, '*_split_*'))\n",
        "\tos.system('rm ' + os.path.join(dataloc, 'D_land.npy'))\n",
        "\tos.system('rm ' + os.path.join(dataloc, 'D_nonland.npy'))\n",
        "\tos.system('rm ' + os.path.join(dataloc, 'curv.txt'))\n",
        "\tos.system('rm ' + os.path.join(dataloc, 'd_nonland_true.txt'))\n",
        "\tos.system('rm ' + os.path.join(dataloc, '*rank*'))\n",
        "\n",
        "def load_data_preproc(nodes, landmarks, dataloc):\n",
        "\t# Load full distance matrix for all nodes\n",
        "\tprint(\"loading testing data...\")\n",
        "\tD_test_points = loadtxt(os.path.join(dataloc, 'test_point_distances.txt'), dtype='int')\n",
        "\tD_test_pairs = loadtxt(os.path.join(dataloc, 'test_points.txt'), dtype='int')\n",
        "\n",
        "\t# define landmark ids and get distance matrix from only landmarks to nodes\n",
        "\tprint(\"loading land_ids...\")\n",
        "\tland_ids = loadtxt(os.path.join(dataloc, 'landmark_ids.txt'), dtype='int32')\n",
        "\tD_land2nodes = load(os.path.join(dataloc, 'dist_mtx_landmarks2nodes.npy'))\n",
        "\n",
        "\tprint(\"finding nonland_ids...\")\n",
        "\tind = ~in1d(range(nodes),land_ids)\n",
        "\tprint(\"length land_ids...\" + str(len(land_ids)))\n",
        "\tprint(\"length ind...\" + str(len(ind)))\n",
        "\tprint(\"length D_land2nodes...\" + str(len(D_land2nodes)))\n",
        "\tD_nonland = D_land2nodes[:,ind]\n",
        "\tnonland_ids = arange(nodes)[ind]\n",
        "\n",
        "\treturn D_land2nodes, D_land2nodes[:,land_ids], land_ids, D_nonland, nonland_ids, D_test_pairs, D_test_points\n",
        "\n",
        "def load_data_main(nodes, landmarks, dataloc=''):\n",
        "\t# Load full distance matrix for all nodes\n",
        "\tD_test_points = loadtxt(os.path.join(dataloc, 'test_point_distances.txt'), dtype='int32')\n",
        "\tD_test_pairs = loadtxt(os.path.join(dataloc, 'test_points.txt'), dtype='int32')\n",
        "\n",
        "\t# define landmark ids and get distance matrix from only landmarks to nodes\n",
        "\tland_ids = loadtxt(os.path.join(dataloc, 'landmark_ids.txt'), dtype='int32')\n",
        "\tD_land = load(os.path.join(dataloc, 'D_land.npy')).astype('int')\n",
        "\n",
        "\tind = ~in1d(range(nodes),land_ids)\n",
        "\tnonland_ids = arange(nodes)[ind]\n",
        "\n",
        "\treturn D_land, land_ids, nonland_ids, D_test_pairs, D_test_points\n",
        " \n",
        "def load_data_4land(nodes, landmarks, dataloc=''):\n",
        "\t# define landmark ids and get distance matrix from only landmarks to nodes\n",
        "\tland_ids = loadtxt(os.path.join(dataloc, 'landmark_ids.txt'), dtype='int32')\n",
        "\tD_land = load(os.path.join(dataloc, 'D_land.npy')).astype('int64')\n",
        "\tind = ~in1d(range(nodes),land_ids)\n",
        "\tnonland_ids = arange(nodes)[ind]\n",
        "\n",
        "\treturn D_land, land_ids, nonland_ids\n",
        "\n",
        "def load_data_4nonland(nodes, landmarks, dataloc=''):\n",
        "\t# define landmark ids and get distance matrix from only landmarks to nodes\n",
        "\tland_ids = loadtxt(os.path.join(dataloc, 'landmark_ids.txt'), dtype='int32')\n",
        "\tind = ~in1d(range(nodes),land_ids)\n",
        "\tnonland_ids = arange(nodes)[ind]\n",
        "\n",
        "\treturn land_ids, nonland_ids\n",
        "\n",
        "def split_nonland_mtx(nodes, landmarks, dataloc, n_split):\n",
        "\tnode_ids = array_split(arange(nodes - landmarks),n_split)\n",
        "\t# load data and do pre-processing\n",
        "\tD_land2nodes, D_land, land_ids, D_nonland, nonland_ids, D_test_pairs0, D_test_dist0 = load_data_preproc(nodes, landmarks, dataloc)\n",
        "\tsave(os.path.join(dataloc, 'D_nonland'), D_nonland)\n",
        "\tsave(os.path.join(dataloc, 'D_land'), D_land)\n",
        "\tfor ii,nn in enumerate(node_ids):\n",
        "\t\tprint(\"Splitting %i out of %i\" %(ii,len(node_ids)))\n",
        "\t\tD_nonland_temp = D_nonland[:,nn]\n",
        "\t\tsave(os.path.join(dataloc, 'D_nonland_split_', str(ii).zfill(3)), D_nonland_temp)\n",
        "\n",
        "\tstemp = sum(D_nonland**2,0)\n",
        "\tprint(stemp[:10])\n",
        "\td_nonland_true = sqrt(abs(stemp)) # norm along rows, \n",
        "\tsavetxt(os.path.join(dataloc, 'd_nonland_true.txt'), d_nonland_true)\n",
        "\n",
        "\treturn 0.0\n",
        "\n",
        "def preproc(nodes, landmarks, dataloc, nprocs, maxprocs4seeds):\n",
        "\tnode_ids = array_split(arange(nodes - landmarks),nprocs)\n",
        "\t# load data and do pre-processing\n",
        "\tprint(\"Loading data...\")\n",
        "\tD_land2nodes, D_land, land_ids, D_nonland, nonland_ids, D_test_pairs0, D_test_dist0 = load_data_preproc(nodes, landmarks, dataloc)\n",
        "\tprint(\"Saving D_land and D_nonland...\")\n",
        "\tprint(\"Data types for nonlandmark and landmark matrices: \", D_nonland.dtype, D_land.dtype)\n",
        "\tsave(os.path.join(dataloc, 'D_nonland'),D_nonland)\n",
        "\tsave(os.path.join(dataloc, 'D_land'), D_land)\n",
        "\tfor ii,nn in enumerate(node_ids):\n",
        "\t\tprint(\"Splitting %i out of %i\" %(ii+1,len(node_ids)))\n",
        "\t\tD_nonland_temp = D_nonland[:,nn]\n",
        "\t\tsave(os.path.join(dataloc, 'D_nonland_split_' + str(ii).zfill(3)), D_nonland_temp)\n",
        "\tns = 20\n",
        "\tls = array_split(range(nodes-landmarks),ns)\n",
        "\t# square and sum chunks to avoid dtype errors (< 0 for int8)\n",
        "\tS = []\n",
        "\tfor ii,lsi in enumerate(ls):\n",
        "\t\tprint(\"squaring nonlandmark matrix %i out of %i\" %(ii,ns))\n",
        "\t\tDtemp = D_nonland[:,lsi].astype('int')\n",
        "\t\tS.append(sum(Dtemp**2,0))\n",
        "\tstemp = hstack(S)\n",
        "\tprint(stemp[stemp < 0])\n",
        "\td_nonland_true = sqrt(stemp) # norm along rows, \n",
        "\tsavetxt(os.path.join(dataloc, 'd_nonland_true.txt'), d_nonland_true)\n",
        "\n",
        "\t# generate random seed list\n",
        "\tprint(\"generating random seed list...\")\n",
        "\tseed_list = random.randint(1,max(maxprocs4seeds,1e4),maxprocs4seeds)\n",
        "\tsavetxt(os.path.join(dataloc, 'random_seed_list.txt'), seed_list, fmt='%i')\n",
        "\n",
        "\treturn 0.0\n",
        "\n",
        "# **********************************\n",
        "# Objective function definitions\n",
        "# **********************************\n",
        "\n",
        "def f(X,Y):\n",
        "\treturn .5*linalg.norm(X-Y, ord='fro')**2\n",
        "\n",
        "def df(X,Y):\n",
        "\treturn X-Y\n",
        "\n",
        "def hype_dist(p1,p2,curv):\n",
        "\tk = curv\n",
        "\tc = 1./sqrt(abs(k))\n",
        "\tx = dot(p1,p2)\n",
        "\tusq1 = dot(p1,p1) + 1.0\n",
        "\tusq2 = dot(p2,p2) + 1.0\n",
        "\tudotu = x - sqrt(usq1)*sqrt(usq2)\n",
        "\tacoshudotu = real(arccosh(-udotu + 0j))\n",
        "\td = acoshudotu * c\n",
        "\treturn d\n",
        "\n",
        "def land_obj(x,**kwargs):\n",
        "\tdim = kwargs['dim']\n",
        "\tD0 = kwargs['D_landmark']\n",
        "\tgrad = zeros(len(x))\n",
        "\tpoints = len(x[:-1])//dim\n",
        "\tk = x[-1]\n",
        "\tc = 1./sqrt(abs(k))\n",
        "\tP = x[:-1].reshape((points,dim))\n",
        "\n",
        "\tX = dot(P,P.T)\n",
        "\tUsq = diag(X) + 1.0\n",
        "\tU = sqrt(Usq)\n",
        "\tUdotU = X - outer(U,U)\n",
        "\tacoshUdotU = real(arccosh(-UdotU + 0j))\n",
        "\tfill_diagonal(acoshUdotU,0.0)\n",
        "\n",
        "\tD = acoshUdotU*c;\n",
        "\tD[isnan(D)] = 0\n",
        "\tdE = df(D,D0)\n",
        "\terr = f(D,D0)/2.0 # since matrix is symmetric\n",
        "\n",
        "\t# compute gradient w.r.t. k\n",
        "\tgradk = - dE * acoshUdotU * .5 * k * abs(k)**-2.5\n",
        "\tgrad[-1] = sum(sum(gradk))/2.0 # divide by two since matrix is symmetric\n",
        "\n",
        "\t# compute gradient\n",
        "\ttol = 1e-12\n",
        "\tAtemp = UdotU**2 - 1.0\n",
        "\tAtemp[Atemp<=0] = tol\n",
        "\tA = - 1./sqrt(Atemp) * c\n",
        "\tB = outer(1./U,U)\n",
        "\tC = dE * A\n",
        "\tH1 = C * B\n",
        "\thsum = sum(H1,1)\n",
        "\tL = len(D0)\n",
        "\tCi = zeros((len(C),len(C)))\n",
        "\tfor i in range(L):\n",
        "\t\tui = P[i,:]\n",
        "\t\thi = hsum[i]\n",
        "\t\tfill_diagonal(Ci,C[i])\n",
        "\t\tg = sum(dot(P.T,Ci),1) - hi*ui\n",
        "\t\tgrad[dim*i:(i+1)*dim] = g\n",
        "\n",
        "\treturn err, grad\n",
        "\n",
        "def nonland_obj(x, node, **kwargs):\n",
        "\t'''\n",
        "\tcomputes nonlandmark error for an entire nonlandmark\n",
        "\tmatrix D0\n",
        "\t'''\n",
        "\t# node is index of D_nonland not D_land2nodes\n",
        "\tdim = kwargs['dim']\n",
        "\tD0 = kwargs['D_nonland']\n",
        "\tL = kwargs['Land_points']\n",
        "\tn_land, dim = L.shape\n",
        "\tnl_id = node\n",
        "\tk = kwargs['k']\n",
        "\tc = 1./sqrt(abs(k))\n",
        "\n",
        "\tX = dot(L,x)\n",
        "\tu = sqrt(dot(x,x) + 1.0)\n",
        "\tV = sqrt(sum(L.T**2,0) + 1.0)\n",
        "\tudotV = outer(u,V) - X\n",
        "\td = real(arccosh(udotV + 0j)) * c\n",
        "\td0 = D0[:,nl_id]\n",
        "\terr = f(d,d0)\n",
        "\tderr = df(d,d0)\n",
        "\n",
        "\t# compute gradient\n",
        "\ttol = 1e-12\n",
        "\tAtemp = udotV**2 - 1.0\n",
        "\tAtemp[Atemp<=0] = tol\n",
        "\tA = - 1./sqrt(Atemp) * c\n",
        "\tB = outer(1./u,V)\n",
        "\tC = derr * A\n",
        "\tH1 = C * B\n",
        "\tH1.shape = (n_land,1)\n",
        "\tC.shape = (n_land,1)\n",
        "\n",
        "\tui = x\n",
        "\tUi = tile(ui,(n_land,1)).T\n",
        "\tHi = diag(H1[:,0])\n",
        "\tCi = diag(C[:,0])\n",
        "\tG = dot(L.T,Ci) - dot(Ui,Hi)\n",
        "\tgrad = sum(G,1)\n",
        "\n",
        "\treturn err, grad\n",
        "\n",
        "def nonland_obj_percol(x, dim, L, k, d0):\n",
        "\t'''\n",
        "\tcomputes nonlandmark error for fixed column d0 of the \n",
        "\tnonlandmark matrix\n",
        "\t'''\n",
        "\tland_idx = arange(len(L))\n",
        "\tLnew = L[land_idx,:]\n",
        "\tn_land, dim = Lnew.shape\n",
        "\tc = 1./sqrt(abs(k))\n",
        "\n",
        "\tX = dot(Lnew,x)\n",
        "\tu = sqrt(dot(x,x) + 1.0)\n",
        "\tV = sqrt(sum(Lnew.T**2,0) + 1.0)\n",
        "\tudotV = outer(u,V) - X\n",
        "\td = real(arccosh(udotV + 0j)) * c\n",
        "\terr = f(d,d0)\n",
        "\tderr = df(d,d0)\n",
        "\n",
        "\t# compute gradient\n",
        "\ttol = 1e-12\n",
        "\tAtemp = udotV**2 - 1.0\n",
        "\tAtemp[Atemp<=0] = tol\n",
        "\tA = - 1./sqrt(Atemp) * c\n",
        "\tB = outer(1./u,V)\n",
        "\tC = derr * A\n",
        "\tH1 = C * B\n",
        "\tH1.shape = (n_land,1)\n",
        "\tC.shape = (n_land,1)\n",
        "\n",
        "\tui = x\n",
        "\tUi = tile(ui,(n_land,1)).T\n",
        "\tHi = diag(H1[:,0])\n",
        "\tCi = diag(C[:,0])\n",
        "\tG = dot(Lnew.T,Ci) - dot(Ui,Hi)\n",
        "\tgrad = sum(G,1)\n",
        "\n",
        "\treturn err, grad\n",
        "\n",
        "def pre_nonland_obj_percol(x, dim, L, k, rL_in_land_idx, d0):\n",
        "\t'''\n",
        "\tcomputes nonlandmark error for fixed column d0 of the \n",
        "\tnonlandmark matrix\n",
        "\t'''\n",
        "\tLnew = L[rL_in_land_idx,:]\n",
        "\td0 = d0[rL_in_land_idx]\n",
        "\tn_land, dim = Lnew.shape\n",
        "\tc = 1./sqrt(abs(k))\n",
        "\n",
        "\tX = dot(Lnew,x)\n",
        "\tu = sqrt(dot(x,x) + 1.0)\n",
        "\tV = sqrt(sum(Lnew.T**2,0) + 1.0)\n",
        "\tudotV = outer(u,V) - X\n",
        "\td = real(arccosh(udotV + 0j)) * c\n",
        "\terr = f(d,d0)\n",
        "\tderr = df(d,d0)\n",
        "\n",
        "\t# compute gradient\n",
        "\ttol = 1e-12\n",
        "\tAtemp = udotV**2 - 1.0\n",
        "\tAtemp[Atemp<=0] = tol\n",
        "\tA = - 1./sqrt(Atemp) * c\n",
        "\tB = outer(1./u,V)\n",
        "\tC = derr * A\n",
        "\tH1 = C * B\n",
        "\tH1.shape = (n_land,1)\n",
        "\tC.shape = (n_land,1)\n",
        "\n",
        "\tui = x\n",
        "\tUi = tile(ui,(n_land,1)).T\n",
        "\tHi = diag(H1[:,0])\n",
        "\tCi = diag(C[:,0])\n",
        "\tG = dot(Lnew.T,Ci) - dot(Ui,Hi)\n",
        "\tgrad = sum(G,1)\n",
        "\n",
        "\treturn err, grad\n",
        "\n",
        "# **********************************\n",
        "# Landmark Embedding functions\n",
        "# **********************************\n",
        "\n",
        "def embed_prelandmarks(xstart,factr,param_list):\n",
        "\tdef f(x):\n",
        "\t\treturn land_obj(x,**param_list)\n",
        "\tbnds = [ (None, None) for i in range(len(xstart))]\n",
        "\tx,f,d = fmin_l_bfgs_b(f,xstart, bounds=bnds, factr=factr)\n",
        "\treturn x,f,d\n",
        "\t\n",
        "def embed_pre_nonlandmarks(xstart,node,param_list2):\n",
        "\tdef g(x):\n",
        "\t\treturn nonland_obj(x,node,**param_list2)\n",
        "\tbnds = [ (None, None) for i in range(len(xstart))]\n",
        "\tx,f,d = fmin_l_bfgs_b(g,xstart, bounds=bnds, factr=1e12)\n",
        "\treturn x,f,d\n",
        "\n",
        "def pre_embedding(dim, D_land, landmarks0, xstart, seed=133):\n",
        "\tD0 = D_land\n",
        "\tx0 = xstart\n",
        "\tnL = len(D0)\n",
        "\trn = random.RandomState(seed)\n",
        "\tif nL <= 32:\n",
        "\t\tx = append(rn.rand(1), x0.flatten())\n",
        "\t\tparam_list = {'dim': dim, 'D_landmark' : D_land}\n",
        "\t\tx1,_,_ = embed_prelandmarks(x,1e10,param_list)\n",
        "\t\treturn x1\n",
        "\telse:\n",
        "\t\t# find nearest power of two and embed\n",
        "\t\texp = floor(log2(nL))\n",
        "\t\tif abs(exp - log2(nL)) == 0:\n",
        "\t\t\t# if nL is a perfect power of 2, then use lowest exp\n",
        "\t\t\trL = int(2**(exp-1))\n",
        "\t\telse:\n",
        "\t\t\trL = int(2**(exp))\n",
        "\t\tland_ids_new = array(sort(rn.permutation(nL)[:rL])).astype('int')\n",
        "\t\tD_land2nodes_new = D0[land_ids_new,:]\n",
        "\t\tD_land_new = D_land2nodes_new[:,land_ids_new]\n",
        "\t\txstart_new = x0[land_ids_new]\n",
        "\t\t# embed new set of sub-landmarks\n",
        "\t\tparam_list_new = {'dim': dim, 'D_landmark' : D_land_new, 'x_start0' : xstart_new}\n",
        "\t\tx_land_soln = pre_embedding(dim, D_land_new,landmarks0,xstart_new)\n",
        "\t\t# solve exactly (comment out for faster runs)\n",
        "\t\tif rL > 32 and rL <= int(landmarks0/2):\n",
        "\t\t\tx1temp,_,_ = embed_prelandmarks(x_land_soln,1e10,param_list_new)\n",
        "\t\t\tx_land_soln = x1temp\n",
        "\t\tL_new = x_land_soln[:-1].reshape((rL,dim))\n",
        "\t\tcurv_new = x_land_soln[-1]\n",
        "\t\t# embed nonlandmarks\n",
        "\t\tnonland_ids_new = delete(arange(nL),land_ids_new)\n",
        "\t\tD_nonland_new = D_land2nodes_new[:,nonland_ids_new]\n",
        "\t\tnl_ids = range(len(nonland_ids_new))\n",
        "\t\tnonland_xmin = []\n",
        "\t\tfor nl_id in nl_ids:\n",
        "\t\t\tparam_list2_new = {'dim': dim, 'Land_points': L_new, 'D_nonland': D_nonland_new, 'k': curv_new}\n",
        "\t\t\txstart2_0 = x0[nonland_ids_new[nl_id]]\n",
        "\t\t\tx2,_,_ = embed_pre_nonlandmarks(xstart2_0,nl_id,param_list2_new)\n",
        "\t\t\tnonland_xmin.append(x2)\n",
        "\t\t# fill in embedded points\n",
        "\t\tC0 = zeros((nL,dim))\n",
        "\t\tfor ii,id in enumerate(land_ids_new):\n",
        "\t\t\tC0[id,:] = L_new[ii,:]\n",
        "\t\tfor ii,xmin_nl in enumerate(nonland_xmin):\n",
        "\t\t\tC0[nonland_ids_new[ii],:] = xmin_nl\n",
        "\t\treturn append(C0.flatten(),curv_new)\n",
        "\n",
        "def pre_embed(s, param_list):\n",
        "\t''' \n",
        "\tCall pre_embedding \n",
        "\t'''\n",
        "\tdim = param_list['dim']\n",
        "\tD_land = param_list['D_landmark']\n",
        "\tlandmarks0 = param_list['landmarks0']\n",
        "\txstart = param_list['x_start0']\n",
        "\tx = pre_embedding(dim,D_land,landmarks0,xstart,seed=s)\n",
        "\treturn land_obj(x,**param_list)[0], x\n",
        "\n",
        "def embed_landmarks(xstart,param_list,factr=1e10):\n",
        "\tdef f(x):\n",
        "\t\treturn land_obj(x,**param_list)\n",
        "\tbnds = [ (None, None) for i in range(len(xstart))]\n",
        "\tx,f,d = fmin_l_bfgs_b(f,xstart,\\\n",
        "\t\t\t\t\t\t\tbounds=bnds,\\\n",
        "\t\t\t\t\t\t\tfactr=factr)\n",
        "\treturn f,x\n",
        "\n",
        "def F(args):\n",
        "\t'''\n",
        "\tMain embedding algorithm (run for multiple seeds)\n",
        "\t'''\n",
        "\tseed = args[0]\n",
        "\tparam_list = args[1]\n",
        "\tf0,x0 = pre_embed(seed, param_list)\n",
        "\treturn embed_landmarks(x0,param_list)\n",
        "\n",
        "def run_landmark_embedding(param_list, np, nodes, land_ids, dataloc):\n",
        "\tstart = time.time()\n",
        "\trn_test = random.RandomState(163)\n",
        "\tinputs1 = [[rn_test.randint(1e3),param_list] for i in range(np)]\n",
        "\tpool = mp.Pool(processes=np)\n",
        "\tsolns = pool.map(F, inputs1)\n",
        "\tfvals = [output[0] for output in solns]\n",
        "\tmin_index = argmin(fvals)\n",
        "\tfval,xsoln = solns[min_index]\n",
        "\tland_error = fval\n",
        "\tdim = param_list['dim']\n",
        "\tD_land = param_list['D_landmark']\n",
        "\tlandmarks = len(D_land)\n",
        "\tXsoln = xsoln[:-1].reshape(landmarks,dim)\n",
        "\tcsoln = xsoln[-1]\n",
        "\tlandmark_rel_error = sqrt(2*land_error)/linalg.norm(D_land,ord='fro')\n",
        "\tprint(\"Total landmark time is %3.5f seconds\" %(time.time()-start))\n",
        "\tC = zeros((nodes, dim))\n",
        "\tC[land_ids,:] = Xsoln\n",
        "\tsavetxt(os.path.join(dataloc, 'curv.txt'),[csoln])\n",
        "\tsavetxt(os.path.join(dataloc, 'fval.txt'),[fval])\n",
        "\tsave(os.path.join(dataloc, 'C'),C)\n",
        "\tsave(os.path.join(dataloc, 'L_points'),C[land_ids,:])\n",
        "\n",
        "\thypy_land_time = time.time() - start\n",
        "\tsavetxt(os.path.join(dataloc, 'hypy_land_time.txt'),[hypy_land_time])\n",
        "\tsavetxt(os.path.join(dataloc, 'landmark_rel_error.txt'),[landmark_rel_error])\n",
        "\n",
        "\tL_points = C[land_ids]\n",
        "\tcurv = csoln\n",
        "\tpool.close()\n",
        "\n",
        "\treturn hypy_land_time, landmark_rel_error, L_points, curv, C\n",
        "\n",
        "def run_single_landmark_embedding(param_list, nodes, land_ids, dataloc, seed):\n",
        "\tstart = time.time()\n",
        "\n",
        "\tfval, xsoln = F((seed,param_list))\n",
        "\tland_error = fval\n",
        "\tdim = param_list['dim']\n",
        "\tD_land = param_list['D_landmark']\n",
        "\tlandmarks = len(D_land)\n",
        "\tXsoln = xsoln[:-1].reshape(landmarks,dim)\n",
        "\tcsoln = xsoln[-1]\n",
        "\tlandmark_rel_error = sqrt(2*land_error)/linalg.norm(D_land,ord='fro')\n",
        "\thypy_land_time = time.time() - start\n",
        "\n",
        "\treturn hypy_land_time, landmark_rel_error, Xsoln, csoln\n",
        "\n",
        "def gather_landmarks(nodes, landmarks, dim, dataloc, nprocs):\n",
        "\t'''\n",
        "\tGet optimal embedding out of different ranks\n",
        "\t'''\n",
        "\tfvals = zeros(nprocs)\n",
        "\tland_times = zeros(nprocs)\n",
        "\tfor r in range(nprocs):\n",
        "\t\tfvals[r] = loadtxt(os.path.join(dataloc, 'fval_rank' + str(r) + '.txt'))\n",
        "\t\tland_times[r] = loadtxt(os.path.join(dataloc, 'land_time_rank' + str(r) + '.txt'))\n",
        "\topt_r = argmin(fvals) # optimal (min) error\n",
        "\tfopt = fvals[opt_r] # relative error (not actual objective fun)\n",
        "\ttopt = amax(land_times)\n",
        "\tcopt = loadtxt(os.path.join(dataloc, 'curv_rank' + str(opt_r) + '.txt')) # optimal curvature\n",
        "\tland_coord = load(os.path.join(dataloc, 'land_coord_rank' + str(opt_r) + '.npy'))\n",
        "\tsavetxt(os.path.join(dataloc, 'landmark_rel_error.txt'), [fopt])\n",
        "\n",
        "\t# save optimal landmark coordinates and curvature\n",
        "\tsave(os.path.join(dataloc, 'land_coord_opt'), land_coord)\n",
        "\tsavetxt(os.path.join(dataloc, 'copt.txt'), [copt])\n",
        "\tsavetxt(os.path.join(dataloc, 'hypy_curv.txt'), [copt])\n",
        "\tsavetxt(os.path.join(dataloc, 'fopt.txt'), [fopt])\n",
        "\tsavetxt(os.path.join(dataloc, 'hypy_land_time.txt'), [amax(land_times)])\n",
        "\treturn fopt, land_coord, copt, topt, opt_r\n",
        "\n",
        "def gather_nonlandmarks_split(nodes, landmarks, dim, dataloc, split, nprocs):\n",
        "\tF2vals = []\n",
        "\tX2solns = []\n",
        "\tNLtimes = []\n",
        "\tfor r in range(nprocs):\n",
        "\t\tftemp = load(os.path.join(dataloc, 'temp', 'f2s_split' + str(split) + '_rank' + str(r) + '.npy'))\n",
        "\t\txtemp = load(os.path.join(dataloc, 'temp', 'x2s_split' + str(split) + '_rank' + str(r) + '.npy'))\n",
        "\t\ttimetemp = loadtxt(os.path.join(dataloc, 'temp', 'nonland_time_split' + str(split) + '_rank' + str(r) + '.txt'))\n",
        "\t\tF2vals.append(ftemp)\n",
        "\t\tX2solns.append(xtemp)\n",
        "\t\tNLtimes.append(float(timetemp))\n",
        "\tf2s = hstack(F2vals).flatten()\n",
        "\tx2s = vstack(X2solns)\n",
        "\thypy_nonland_time = amax(NLtimes)\n",
        "\n",
        "\tsave(os.path.join(dataloc, 'f2vals_split_' + str(split)), f2s)\n",
        "\tsave(os.path.join(dataloc, 'x2solns_split_' + str(split)), x2s)\n",
        "\tsave(os.path.join(dataloc, 'nonland_time_split_' + str(split)), hypy_nonland_time)\n",
        "\n",
        "\treturn f2s, x2s, hypy_nonland_time\n",
        "\n",
        "def gather_nonlandmarks(nodes, landmarks, dim, dataloc, n_split):\n",
        "\tF2vals = []\n",
        "\tX2solns = []\n",
        "\tNLtimes = []\n",
        "\tfor nn in range(n_split):\n",
        "\t\tftemp = load(os.path.join(dataloc, 'f2vals_split_' + str(nn) + '.npy'))\n",
        "\t\txtemp = load(os.path.join(dataloc, 'x2solns_split_' + str(nn) + '.npy'))\n",
        "\t\ttimetemp = load(os.path.join(dataloc, 'nonland_time_split_' + str(nn) + '.npy'))\n",
        "\t\tF2vals.append(ftemp)\n",
        "\t\tX2solns.append(xtemp)\n",
        "\t\tNLtimes.append(float(timetemp))\n",
        "\n",
        "\tf2s = hstack(F2vals).flatten()\n",
        "\tx2s = vstack(X2solns)\n",
        "\thypy_nonland_time = sum(NLtimes)\n",
        "\n",
        "\td_nonland_true = loadtxt(os.path.join(dataloc, 'd_nonland_true.txt'))\n",
        "\tnonland_avg_rel_error = mean(sqrt(2*f2s).flatten()/d_nonland_true)\n",
        "\n",
        "\tprint(\"Total nonlandmark time is %3.5f seconds\" %hypy_nonland_time)\n",
        "\t\n",
        "\t# add nonland coordinates to main embedding\n",
        "\tland_ids = loadtxt(os.path.join(dataloc, 'landmark_ids.txt')).astype('int')\n",
        "\tnonland_ids = loadtxt(os.path.join(dataloc, 'nonlandmark_ids.txt')).astype('int')\n",
        "\tland_coord = load(os.path.join(dataloc, 'land_coord_opt.npy'))\n",
        "\tC = zeros((nodes, dim))\n",
        "\tC[land_ids,:] = land_coord\n",
        "\tC[nonland_ids,:] = x2s\n",
        "\tsave(os.path.join(dataloc, 'hypy_coord'), C)\n",
        "\tsavetxt(os.path.join(dataloc, 'hypy_nonland_time.txt'), [hypy_nonland_time])\n",
        "\tsavetxt(os.path.join(dataloc, 'nonland_avg_rel_error.txt'), [nonland_avg_rel_error])\n",
        "\n",
        "\tprint(\"Non-landmark error: \", nonland_avg_rel_error)\n",
        "\treturn C, nonland_avg_rel_error, hypy_nonland_time\n",
        "\n",
        "def cleanup_at_end(dataloc):\n",
        "\tprint(\"Cleaning up data folder...\")\n",
        "\tos.system('rm ' + os.path.join(dataloc, 'temp/*.*'))\n",
        "\tos.system('rm ' + os.path.join(dataloc, '*opt*'))\n",
        "\tos.system('rm ' + os.path.join(dataloc, 'd_nonland_true.txt'))\n",
        "\tos.system('rm ' + os.path.join(dataloc, '*split*'))\n",
        "\tos.system('rm ' + os.path.join(dataloc, '*rank*'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NMlxDc8Irxo1"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import numpy.random as random\n",
        "import matplotlib.pyplot as mpl\n",
        "import multiprocessing as mp\n",
        "import pandas as pd\n",
        "import os, pdb, time, pickle, sys, argparse\n",
        "\n",
        "class Preprocessing:\n",
        "\n",
        "\tdef __init__(self, name, indir, outdir, nprocs, nL, nVal):\n",
        "\t\tprint(\"Start Preprocessing\")\n",
        "\t\tself.name = name # network name\n",
        "\t\tself.location = indir # directory for input data (contains edge list from snap)\n",
        "\t\tself.datadir = outdir # directory for output data\n",
        "\t\tself.nprocs = nprocs # number of processes for parallel embedding\n",
        "\t\tself.nL = nL # number of landmarks\n",
        "\t\tself.nVal = nVal # number of validation pairs\n",
        "\t\n",
        "\tdef ReMapNodes(self, filename):\n",
        "\n",
        "\t\tgraphsrc = filename # original file from SNAP\n",
        "\t\tnprocs = self.nprocs # processes\n",
        "\t\tdirloc = self.location\n",
        "\t\tdatafile = dirloc + graphsrc\n",
        "\t\tnewfilename = dirloc + graphsrc + '_remapped' # remapped edge list\n",
        "\n",
        "\t\tedge_list, nodes, n_edges = self.load_graph_data(datafile)\n",
        "\n",
        "\t\t# create dictionary map from old nodes to new nodes\n",
        "\t\tglobal D\n",
        "\t\tD = {}\n",
        "\t\tfor i,n in enumerate(nodes):\n",
        "\t\t\tD[n] = i\n",
        "\n",
        "\t\t# remap function requires dictionary D as a global\n",
        "\t\tstart = time.time()\n",
        "\t\tnew_edge_list = self.remap_edge_list(edge_list, nprocs)\n",
        "\t\tend = time.time()\n",
        "\t\tprint(\"Total time for remapping is %f seconds\" %(end-start))\n",
        "\n",
        "\t\t# save data\n",
        "\t\tprint('writing to text file...')\n",
        "\t\tnp.savetxt(newfilename,new_edge_list,fmt='%i',delimiter='\\t')\n",
        "\n",
        "\t\tself.edge_list_file = filename + '_remapped'\n",
        "\t\tself.edge_list_location = self.location\n",
        "\t\t\n",
        "\tdef GenAdjMat(self, filename):\n",
        "\t\t'''\n",
        "\t\tCompute the landmarks given the edge list along\n",
        "\t\twith distances from nodes to each landmark\n",
        "\t\t'''\n",
        "\t\tself.metadata = {\"nL\":self.nL, \"nVal\":self.nVal, \"datadir\":self.datadir}\n",
        "\n",
        "\t\tos.system('python lm_emb_preprocessing_gen_adj.py' + ' -filename=' + filename  \\\n",
        "\t\t\t\t\t\t\t\t\t\t\t+ ' -location=' + self.location  \\\n",
        "\t\t\t\t\t\t\t\t\t\t\t+ ' -datadir=' + self.datadir   \\\n",
        "\t\t\t\t\t\t\t\t\t\t\t+ ' -nprocs=' + str(self.nprocs) \\\n",
        "\t\t\t\t\t\t\t\t\t\t\t+ ' -nL=' + str(self.nL) \\\n",
        "\t\t\t\t\t\t\t\t\t\t\t+ ' -nval=' + str(self.nVal))\n",
        "\t\t\t\t\t\t\t\t\t\t\t\n",
        "\t\tself.nnodes = np.int(np.loadtxt(self.datadir + 'nNodes.txt'))\n",
        "\t\tself.metadata[\"nnodes\"] = self.nnodes\n",
        "\t\tself.save_meta(self.metadata, \"metadata\")\n",
        "\t\t\n",
        "\tdef set_main_env(self, maxprocs4seed, split, nodes_per_run):\n",
        "\t\t'''\n",
        "\t\tSplit the nonlandmark embedding into serial chunks\n",
        "\n",
        "\t\tIf split = True, then user has to define nodes_per_run. Split\n",
        "\t\tdefines how many serial chunks to perform the nonlandmark\n",
        "\t\tembedding. Each chunk is then performed in parallel. E.g., \n",
        "\t\tif nnodes = 100 and nodes per run is 2, then the first 50\n",
        "\t\tnodes are embedding in parallel, followed by the next 50.\n",
        "\n",
        "\t\tMost data is written to the datadir so that there is minimal\n",
        "\t\tsharing of data and each function can be run at different times.  \n",
        "\t\t'''\n",
        "\t\t# set hyperbolic dimension and processors for nonland embedding\n",
        "\t\tnodes = self.metadata['nnodes']\n",
        "\t\tlandmarks = self.metadata['nL']\n",
        "\t\tdatadir = self.metadata['datadir']\n",
        "\t\tself._nprocs1 = maxprocs4seed # parallel landmark procs\n",
        "\n",
        "\t\tif split==True:\n",
        "\t\t\tself.nl_split = int(np.ceil((nodes-landmarks)/float(nodes_per_run)))\n",
        "\t\t\t# preprocess data for landmark and nonlandmark embedding\n",
        "\t\t\tos.system('mkdir ' + os.path.join(datadir, 'temp'))\n",
        "\t\t\tlm_emb_func.preprocdata(nodes, landmarks, nodes_per_run, datadir, maxprocs4seed=100*self._nprocs1, cleanup=True)\n",
        "\t\t\n",
        "\tdef save_meta(self, obj, name):\n",
        "\t    with open(os.path.join(self.datadir, name + \".pkl\"), 'wb') as f:\n",
        "\t    \tpickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\tdef load_meta(self, name):\n",
        "\t    with open(os.path.join(self.datadir, name + \".pkl\"), 'rb') as f:\n",
        "\t    \treturn pickle.load(f)\n",
        "\t\n",
        "\tdef split(self, seq, procs):\n",
        "\t\t'''\n",
        "\t\tfor parallel proccessing of re-labeling\n",
        "\t\tseq is a list of input parameters\n",
        "\t\tprocs are the number of processors\n",
        "\t\t'''\n",
        "\t\tavg = len(seq) / float(procs)\n",
        "\t\tout = []\n",
        "\t\tlast = 0.0\n",
        "\t\twhile last < len(seq):\n",
        "\t\t\tout.append(seq[int(last):int(last + avg)])\n",
        "\t\t\tlast += avg\n",
        "\t\treturn out\n",
        "    \t\n",
        "\tdef load_graph_data(self, filename):\n",
        "\t\tprint('Loading data...')\n",
        "\t\te0 = pd.read_csv(filename,sep='\\t',header=None,skiprows=4,dtype='int32')\n",
        "\t\te1 = np.array(e0[0],dtype='int32')   \n",
        "\t\te1.shape = (len(e1),1)\n",
        "\t\te2 = np.array(e0[1],dtype='int32')\n",
        "\t\te2.shape = (len(e2),1)   \n",
        "\t\tedge_list = np.hstack((e1,e2)).astype('int32')\n",
        "\t\tn_edges = len(edge_list)\n",
        "\t\tnodes = np.unique(edge_list.flatten()) # get number of nodes\n",
        "\t\tprint('Done loading data!')\n",
        "\n",
        "\t\treturn edge_list, nodes, n_edges\n",
        "\n",
        "\tdef get_new_edge_list(self, edge_list_temp):\n",
        "\t\t''' \n",
        "\t\trenumber edge list using dictionary of indices\n",
        "\t\t'''\n",
        "\t\tnew_edge_list = np.zeros(edge_list_temp.shape,dtype='int32')\n",
        "\t\tfor j,r in enumerate(edge_list_temp):\n",
        "\t\t\tif j % 100000 == 0: print(\"%.2f percent done\" %(float(j)/len(edge_list_temp)))\n",
        "\t\t\tnew_edge_list[j,0] = D[r[0]]\n",
        "\t\t\tnew_edge_list[j,1] = D[r[1]]\n",
        "\t\treturn new_edge_list\n",
        "\n",
        "\tdef remap_edge_list(self, edge_list, nprocs=4):\n",
        "\t\t''' \n",
        "\t\tre-map edge list to have consecutive numbers (in parallel)\n",
        "\t\tcreate a dictionary which maps node # to index \n",
        "\t\tnote that the node number might be higher than the total number of \n",
        "\t\tunique n_nodes, which is why we are doing this\n",
        "\t\t'''\n",
        "\t\tsp = self.split(range(len(edge_list)),nprocs)\n",
        "\t\tedge_split = [edge_list[s,:] for s in sp]\n",
        "\t\tpool = mp.Pool(processes=nprocs)\n",
        "\t\tnew_edge_lists = pool.map(self.get_new_edge_list,edge_split)\n",
        "\t\tnew_edge_list = np.vstack(new_edge_lists)\n",
        "\n",
        "\t\treturn new_edge_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kB1XBkslpdaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ef8559-5ac3-4686-cf8a-c246b603b412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Preprocessing\n"
          ]
        }
      ],
      "source": [
        "preproc = Preprocessing(name, snapdir, savedir, nprocs, nlandmarks, nvalidation)\n",
        "preproc.metadata = metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zyXh94JBpxk3"
      },
      "outputs": [],
      "source": [
        "def clean(dataloc):\n",
        "    print(\"Cleaning up data folder...\")\n",
        "    os.system('rm ' + os.path.join(dataloc, '*_split_*'))\n",
        "    os.system('rm ' + os.path.join(dataloc, 'D_land.npy'))\n",
        "    os.system('rm ' + os.path.join(dataloc, 'D_nonland.npy'))\n",
        "    os.system('rm ' + os.path.join(dataloc, 'curv.txt'))\n",
        "    os.system('rm ' + os.path.join(dataloc, 'd_nonland_true.txt'))\n",
        "    os.system('rm ' + os.path.join(dataloc, '*rank*'))\n",
        " \n",
        "def preprocdata(nodes, landmarks, nodes_per_run, dataloc, maxprocs4seed=1000, cleanup=False):\n",
        "    print(\"\\n\", \"*\"*60)\n",
        "    print(\"Pre-processing: splitting D mtx for n = %i and L = %i\" %(nodes, landmarks))\n",
        "    print(\"*\"*60)\n",
        "\n",
        "    if cleanup: clean(dataloc)\n",
        "    \n",
        "    # calculate the number of partitions for the D matrix\n",
        "    n_split = int(ceil((nodes-landmarks)/float(nodes_per_run)))\n",
        "    savetxt(dataloc + '/nonland_nsplit.txt', [n_split],fmt='%i')\n",
        "    print(\"Splitting nonlandmark matrix\")\n",
        "    preproc(nodes, landmarks, dataloc, n_split, maxprocs4seed)\n",
        "\n",
        "def preproc(nodes, landmarks, dataloc, nprocs, maxprocs4seeds):\n",
        "    node_ids = array_split(arange(nodes - landmarks),nprocs)\n",
        "    # load data and do pre-processing\n",
        "    print(\"Loading data...\")\n",
        "    land_ids = [i for i in range(nlandmarks)]\n",
        "    non_land_ids = [i for i in range(nlandmarks,nNodes)]\n",
        "    D_land2nodes, D_land, land_ids, D_nonland, nonland_ids = L2n,L2n[:,:nlandmarks],land_ids,L2n[:,nlandmarks:],non_land_ids \n",
        "    print(\"Shape of D_land\",D_land.shape)\n",
        "    print(\"Shape of D_nonland\",D_nonland.shape)\n",
        "    print(\"Saving D_land and D_nonland...\")\n",
        "    print(\"Data types for nonlandmark and landmark matrices: \", D_nonland.dtype, D_land.dtype)\n",
        "    save(os.path.join(dataloc, 'D_nonland'),D_nonland)\n",
        "    save(os.path.join(dataloc, 'D_land'), D_land)\n",
        "    for ii,nn in enumerate(node_ids):\n",
        "        print(\"Splitting %i out of %i\" %(ii+1,len(node_ids)))\n",
        "        D_nonland_temp = D_nonland[:,nn]\n",
        "        save(os.path.join(dataloc, 'D_nonland_split_' + str(ii).zfill(3)), D_nonland_temp)\n",
        "    ns = 20\n",
        "    ls = array_split(range(nodes-landmarks),ns)\n",
        "    # square and sum chunks to avoid dtype errors (< 0 for int8)\n",
        "    S = []\n",
        "    for ii,lsi in enumerate(ls):\n",
        "        print(\"squaring nonlandmark matrix %i out of %i\" %(ii,ns))\n",
        "        Dtemp = D_nonland[:,lsi].astype('int')\n",
        "        S.append(sum(Dtemp**2,0))\n",
        "    stemp = hstack(S)\n",
        "    print(stemp[stemp < 0])\n",
        "    d_nonland_true = sqrt(stemp) # norm along rows, \n",
        "    savetxt(os.path.join(dataloc, 'd_nonland_true.txt'), d_nonland_true)\n",
        "\n",
        "    # generate random seed list\n",
        "    print(\"generating random seed list...\")\n",
        "    seed_list = random.randint(1,max(maxprocs4seeds,1e4),maxprocs4seeds)\n",
        "    savetxt(os.path.join(dataloc, 'random_seed_list.txt'), seed_list, fmt='%i')\n",
        "\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gBUW-HgDqJCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de762353-b54a-4e19-d724-08451a49996d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ************************************************************\n",
            "Pre-processing: splitting D mtx for n = 250 and L = 101\n",
            "************************************************************\n",
            "Cleaning up data folder...\n",
            "Splitting nonlandmark matrix\n",
            "Loading data...\n",
            "Shape of D_land (101, 101)\n",
            "Shape of D_nonland (101, 149)\n",
            "Saving D_land and D_nonland...\n",
            "Data types for nonlandmark and landmark matrices:  float64 float64\n",
            "Splitting 1 out of 4\n",
            "Splitting 2 out of 4\n",
            "Splitting 3 out of 4\n",
            "Splitting 4 out of 4\n",
            "squaring nonlandmark matrix 0 out of 20\n",
            "squaring nonlandmark matrix 1 out of 20\n",
            "squaring nonlandmark matrix 2 out of 20\n",
            "squaring nonlandmark matrix 3 out of 20\n",
            "squaring nonlandmark matrix 4 out of 20\n",
            "squaring nonlandmark matrix 5 out of 20\n",
            "squaring nonlandmark matrix 6 out of 20\n",
            "squaring nonlandmark matrix 7 out of 20\n",
            "squaring nonlandmark matrix 8 out of 20\n",
            "squaring nonlandmark matrix 9 out of 20\n",
            "squaring nonlandmark matrix 10 out of 20\n",
            "squaring nonlandmark matrix 11 out of 20\n",
            "squaring nonlandmark matrix 12 out of 20\n",
            "squaring nonlandmark matrix 13 out of 20\n",
            "squaring nonlandmark matrix 14 out of 20\n",
            "squaring nonlandmark matrix 15 out of 20\n",
            "squaring nonlandmark matrix 16 out of 20\n",
            "squaring nonlandmark matrix 17 out of 20\n",
            "squaring nonlandmark matrix 18 out of 20\n",
            "squaring nonlandmark matrix 19 out of 20\n",
            "[]\n",
            "generating random seed list...\n"
          ]
        }
      ],
      "source": [
        "datadir = metadata['datadir']\n",
        "nprocs1 = maxprocs4seed\n",
        "preprocdata(nNodes, nlandmarks, nodes_per_run, datadir, maxprocs4seed=100*nprocs1, cleanup=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8BnZeJBkLkF0"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import os, math, warnings\n",
        "\n",
        "from scipy.linalg import eigh\n",
        "from scipy.optimize import minimize\n",
        "class LHydra:\n",
        "\n",
        "  def __init__(self, name, indir, outdir, nprocs):\n",
        "    print(\"Start Hydra-Embedding\")\n",
        "    self.name = name\n",
        "    self.snapdir = indir\n",
        "    self.datadir = outdir\n",
        "    self.nprocs = nprocs\n",
        "    \n",
        "  def hydra_landmark(self, dim=2, curvature=1.0, alpha=1.1, equi_adj=0.5, polar=False, isotropic_adj=True, hydra=False, lorentz=False):\n",
        "    ''' \n",
        "    Wrapper function for different variants of the L-hydra method.\n",
        "    \n",
        "    If the curvature is 'None', L-hydra tries to find the optimal curvature.\n",
        "    Otherwise, L-hydra is run with fixed curvature.\n",
        "    '''\n",
        "    if curvature != None:\n",
        "      return self.hydra_landmark_fixed_curvature(curvature, dim, alpha, equi_adj, polar, isotropic_adj, hydra, lorentz)\n",
        "    else:\n",
        "      # setup control parameter for curvature optimization\n",
        "      hydra=False\n",
        "      lorentz = False\n",
        "      \n",
        "      # initial value for curvature \n",
        "      curvature = 1\n",
        "      \n",
        "      eps = np.finfo(np.double).eps\n",
        "      D_land = np.load(os.path.join(self.datadir, 'D_land.npy')).astype('int')\n",
        "      k_bounds = [eps,max(1,(8/D_land.max())**2)] # a priori bounds for curvature\n",
        "      k_min = minimize(\n",
        "        fun=self.hydra_landmark_fixed_curvature,\n",
        "        args=(dim, alpha, equi_adj, polar, isotropic_adj, hydra, lorentz),\n",
        "        x0=curvature,\n",
        "        method=\"BFGS\",\n",
        "        bounds=k_bounds,\n",
        "        options={\"disp\": True, \"maxiter\": 1000}\n",
        "      )\n",
        "      k1_objective = self.hydra_landmark_fixed_curvature(1, dim, alpha, equi_adj, polar, isotropic_adj, hydra, lorentz)\n",
        "      k_optimal = k_min.x[0]\n",
        "      if k1_objective < k_min.fun: \n",
        "        k_optimal = 1 # make sure that returned result is never worse than unit curvature\n",
        "      return self.hydra_landmark_fixed_curvature(k_optimal, dim, alpha, equi_adj, polar, isotropic_adj, hydra, lorentz)\n",
        "        \n",
        "  def hydra_landmark_fixed_curvature(self, curvature=1.0, dim=2, alpha=1.1, equi_adj=0.5, polar=False, isotropic_adj=True, hydra=False, lorentz=False):\n",
        "    \n",
        "    # load distance matrices and landmark indices (as computed during pre-processing)\n",
        "    D_land = np.load(os.path.join(self.datadir, 'D_land.npy')).astype('int') # distances between landmark nodes\n",
        "    D_nonland = np.load(os.path.join(self.datadir, 'D_nonland.npy')).astype('int') # distances between landmark and non-landmark nodes\n",
        "    print(\"shape of dland\", D_land.shape)\n",
        "    print(\"shape of dnonland\", D_nonland.shape)\n",
        "\n",
        "    nodes = np.loadtxt(os.path.join(self.datadir, 'nNodes.txt'),dtype='int32') # number of network nodes\n",
        "    land_ids = np.loadtxt(os.path.join(self.datadir, 'landmark_ids.txt'),dtype='int32') # indices of landmark nodes\n",
        "    \n",
        "    ind = ~np.in1d(range(nodes),land_ids)\n",
        "    nonland_ids = np.arange(nodes)[ind] # indices of non-landmark nodes\n",
        "\n",
        "    # sanitize/check input\n",
        "    if any(np.diag(D_land) != 0):  # non-zero diagonal elements are set to zero\n",
        "      np.fill_diagonal(D_land, 0)\n",
        "      warnings.warn(\"Diagonal of input matrix D_land has been set to zero\")\n",
        "      \n",
        "    if dim > len(D_land):\n",
        "      raise RuntimeError(\n",
        "        f\"Hydra cannot embed {len(D_land)} points in {dim}-dimensions. Limit of {len(D_land)}.\"\n",
        "      )\n",
        "    \n",
        "    if not np.allclose(D_land, np.transpose(D_land)):\n",
        "      warnings.warn(\n",
        "        \"Input matrix D_land is not symmetric.\\\n",
        "        Lower triangle part is used.\"\n",
        "      )\n",
        "\n",
        "    if dim > 2:\n",
        "      # set default values in dimension > 2\n",
        "      isotropic_adj = False\n",
        "      if polar:\n",
        "        warnings.warn(\"Polar coordinates only valid in dimension two\")\n",
        "        polar = False\n",
        "      if equi_adj != 0.0:\n",
        "        warnings.warn(\"Equiangular adjustment only possible in dimension two.\")\n",
        "      \n",
        "    # convert distance matrix to 'hyperbolic Gram matrix'\n",
        "    A_land = np.cosh(np.sqrt(abs(curvature))*D_land)\n",
        "    print(\"shape of A_land\", A_land.shape)\n",
        "    A_nonland = np.cosh(np.sqrt(abs(curvature))*D_nonland)\n",
        "    print(\"shape of A_nonland\", A_nonland.shape)\n",
        "    nlm = len(land_ids)\n",
        "    nnonlm = nodes - nlm\n",
        "    \n",
        "    # check for large/infinite values\n",
        "    A_max = np.amax(A_land)\n",
        "    if A_max > 1e8:\n",
        "      warnings.warn(\n",
        "        \"Gram Matrix contains values > 1e8. Rerun with smaller\\\n",
        "        curvature parameter or rescaled distances.\"\n",
        "      )\n",
        "    if A_max == float(\"inf\"):\n",
        "      warnings.warn(\n",
        "        \"Gram matrix contains infinite values.\\\n",
        "        Rerun with smaller curvature parameter or rescaled distances.\"\n",
        "      )\n",
        "\n",
        "    # Eigendecomposition of A\n",
        "    # compute leading Eigenvalue and Eigenvector\n",
        "    lambda0, x0 = eigh(A_land, subset_by_index=[nlm-1, nlm-1])\n",
        "    # compute lower tail of spectrum\n",
        "    w, v = eigh(A_land, subset_by_index=[0,dim-1])\n",
        "    print(\"w\",len(w))\n",
        "    print(\"v\", len(v))\n",
        "    idx = w.argsort()[::-1]\n",
        "    # print(\"idx\",len(idx))\n",
        "    spec_tail = w[idx] # Last dim Eigenvalues\n",
        "    print(\"\")\n",
        "    X_land_raw = v[:,idx] # Last dim Eigenvectors\n",
        "    print(\"shape of  X_land_raw\",  X_land_raw.shape) \n",
        "    \n",
        "    x0 = x0 * np.sqrt(lambda0) # scale by Eigenvalue\n",
        "    if x0[0]<0:\n",
        "      x0 = -x0 # Flip sign if first element negative\n",
        "\n",
        "    # no isotropic adjustment: rescale Eigenvectors by Eigenvalues\n",
        "    if not isotropic_adj:\n",
        "      if np.array([spec_tail > 0]).any():\n",
        "        warnings.warn(\n",
        "          \"Spectral Values have been truncated to zero. Try to use\\\n",
        "          lower embedding dimension\"\n",
        "        )\n",
        "        spec_tail[spec_tail > 0] = 0\n",
        "      X_land_raw = np.matmul(X_land_raw, np.diag(np.sqrt(np.maximum(-spec_tail,0))))\n",
        "      \n",
        "    X_nonland = np.matmul(np.transpose(A_nonland), np.c_[(x0/lambda0), -(X_land_raw/abs(spec_tail))])\n",
        "    \n",
        "    X_raw = np.zeros((nodes, dim))\n",
        "\n",
        "    print(len(land_ids))\n",
        "    X_raw[land_ids,:] = X_land_raw\n",
        "    \n",
        "    X_raw[nonland_ids,:] = X_nonland[:,1:(dim+1)]\n",
        "    \n",
        "    x0_full = np.zeros((nodes, 1))\n",
        "    x0_full[land_ids,0] = x0[:,0]\n",
        "    x0_full[nonland_ids,0] = X_nonland[:,0]\n",
        "    x_min = x0_full.min()\n",
        "    \n",
        "    \n",
        "    if hydra:\n",
        "      # Calculate radial coordinate\n",
        "      s = np.sqrt(np.sum(X_raw ** 2, axis=1))\n",
        "      directional = X_raw / s[:, None]  # convert to directional coordinates\n",
        "      r = np.sqrt((alpha*x0_full - x_min)/(alpha*x0_full + x_min)) ## multiplicative adjustment (scaling)\n",
        "      X = self.poincare_to_hyper(r=r, directional=directional)\n",
        "    else:\n",
        "      X = X_raw\n",
        "      \n",
        "    # Calculate polar coordinates if dimension is 2\n",
        "    if dim == 2:\n",
        "      # calculate polar angle\n",
        "      theta = np.arctan2(X[:, 0], -X[:, 1])\n",
        "\n",
        "      # Equiangular adjustment\n",
        "      if equi_adj > 0.0:\n",
        "        angles = [(2 * x / nodes - 1) * math.pi for x in range(0, nodes)]\n",
        "        theta_equi = np.array(\n",
        "          [x for _, x in sorted(zip(theta, angles))]\n",
        "        )  # Equi-spaced angles\n",
        "        # convex combination of original and equi-spaced angles\n",
        "        theta = (1 - equi_adj) * theta + equi_adj * theta_equi\n",
        "        # update directional coordinate\n",
        "        directional = np.array([np.cos(theta), np.sin(theta)]).transpose()\n",
        "        \n",
        "    if lorentz:\n",
        "      X_lorentz = np.concatenate((x0_full, X), axis=1)\n",
        "    \n",
        "    np.save(os.path.join(self.datadir, 'xstart'), X)\n",
        "    print('*** DIMENSIONS OF XSTART ' + str(X.shape[0]) + ' and ' + str(X.shape[1]))\n",
        "    self.split_xstart(nodes, nlm, self.datadir, self.nprocs)\n",
        "    \n",
        "    #stress = self.get_stress(curvature, X_raw[land_ids,:], D_land)\n",
        "    \n",
        "    return X\n",
        "    \n",
        "  def split_xstart(self, nodes, landmarks, dataloc, nprocs):\n",
        "    '''\n",
        "    Splits (hyperbolic) coordinates into nprocs chunks of equal size\n",
        "    '''\n",
        "    node_ids = np.array_split(np.arange(nodes - landmarks),nprocs)\n",
        "    print(\"Loading hydra results...\")\n",
        "    xstart_full = np.load(os.path.join(dataloc, 'xstart.npy'))\n",
        "    nonland_ids = np.loadtxt(os.path.join(dataloc, 'nonlandmark_ids.txt'), dtype='int32')\n",
        "    xstart = xstart_full[nonland_ids,:]\n",
        "    print(\"Length of starting point:\" + str(len(xstart)))\n",
        "    for ii,nn in enumerate(node_ids):\n",
        "      print(\"Splitting %i out of %i\" %(ii+1,len(node_ids)))\n",
        "      xstart_tmp = xstart[nn,:]\n",
        "      np.save(os.path.join(dataloc, 'xstart_split_' + str(ii).zfill(3)), xstart_tmp)\n",
        "\n",
        "  def get_stress(self, curvature, x, dist):\n",
        "    '''\n",
        "    Calculate stress of embedding from given coordinates\n",
        "    '''\n",
        "    X = np.matmul(x, x.T)\n",
        "    u_tilde = np.sqrt(X.diagonal() + 1)\n",
        "    H = X - np.outer(u_tilde, u_tilde)\n",
        "    D = 1 / np.sqrt(abs(curvature)) * np.arccosh(np.maximum(-H, 1))\n",
        "    np.fill_diagonal(D, 0)\n",
        "    y = 0.5 * np.sum((D - dist) ** 2)\n",
        "    return y\n",
        "    \n",
        "  def poincare_to_hyper(self, r, directional):\n",
        "    '''\n",
        "    Convert coordinates in the Poincare ball to reduced hyperbolic coordinates\n",
        "    '''\n",
        "    X = (directional.transpose()*(2*r/(1-r**2))).transpose()\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "i-qjI7Zkq6pT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11157679-08e7-44af-9589-d654f72761bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Hydra-Embedding\n",
            "shape of dland (101, 101)\n",
            "shape of dnonland (101, 149)\n",
            "shape of A_land (101, 101)\n",
            "shape of A_nonland (101, 149)\n",
            "w 100\n",
            "v 101\n",
            "\n",
            "shape of  X_land_raw (101, 100)\n",
            "101\n",
            "*** DIMENSIONS OF XSTART 250 and 100\n",
            "Loading hydra results...\n",
            "Length of starting point:149\n",
            "Splitting 1 out of 4\n",
            "Splitting 2 out of 4\n",
            "Splitting 3 out of 4\n",
            "Splitting 4 out of 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-354cfca85f42>:86: UserWarning: Equiangular adjustment only possible in dimension two.\n",
            "  warnings.warn(\"Equiangular adjustment only possible in dimension two.\")\n",
            "<ipython-input-15-354cfca85f42>:130: UserWarning: Spectral Values have been truncated to zero. Try to use          lower embedding dimension\n",
            "  warnings.warn(\n",
            "<ipython-input-15-354cfca85f42>:137: RuntimeWarning: invalid value encountered in true_divide\n",
            "  X_nonland = np.matmul(np.transpose(A_nonland), np.c_[(x0/lambda0), -(X_land_raw/abs(spec_tail))])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.74775388,\n",
              "         2.56038753,  0.86584783],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.78096437,\n",
              "         3.28448747, -3.60999046],\n",
              "       [ 0.        ,  0.        ,  0.        , ..., -1.93837997,\n",
              "         2.01879049,  0.70188553],\n",
              "       ...,\n",
              "       [        nan,         nan,         nan, ..., -0.06641681,\n",
              "        -0.05190234,  0.09040573],\n",
              "       [        nan,         nan,         nan, ..., -0.06641681,\n",
              "        -0.05190234,  0.09040573],\n",
              "       [        nan,         nan,         nan, ..., -0.06641681,\n",
              "        -0.05190234,  0.09040573]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# from lm_emb_hydra import LHydra\n",
        "\n",
        "hydra = LHydra(name, snapdir, savedir, nprocs)\n",
        "hydra.hydra_landmark(dim, curvature, alpha, equi_adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YxR5P4qbsQjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9de0e3b-7c13-4229-c736-8f6a2865c1f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start HydraPlus-Embedding\n"
          ]
        }
      ],
      "source": [
        "from lm_emb_hydra_plus import LHydraPlus\n",
        "# Run Hydra Plus\n",
        "hydra_plus = LHydraPlus(name, savedir, nlandmarks, nodes_per_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Z8q899lYtR2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a00bb6f-4510-48fc-f1a0-2c51e015878e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Hyperbolic dimension for landmark embedding is 100.\n",
            "Complete landmark time is  199.77310347557068\n",
            "Best embedding error is 0.005476\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.25201863,  0.        ,  0.        , ...,  0.03271873,\n",
              "          0.37167976,  0.6916859 ],\n",
              "        [-0.6415289 ,  0.        ,  0.        , ...,  0.45279471,\n",
              "         -0.11406318,  0.33782216],\n",
              "        [-0.53185892,  0.        ,  0.        , ...,  0.33654164,\n",
              "         -0.14094454,  0.44296553],\n",
              "        ...,\n",
              "        [-0.19524659,  0.        ,  0.        , ...,  0.58674867,\n",
              "         -0.32697834,  0.19267922],\n",
              "        [-0.49629982,  0.        ,  0.        , ...,  0.2174847 ,\n",
              "          0.05426408,  0.09499987],\n",
              "        [-0.3020881 ,  0.        ,  0.        , ...,  0.16196927,\n",
              "          0.35069668,  0.57690008]]),\n",
              " 0.09109578037356222,\n",
              " array([[-0.25201863,  0.        ,  0.        , ...,  0.37167976,\n",
              "          0.6916859 ,  1.96621019],\n",
              "        [-0.6415289 ,  0.        ,  0.        , ..., -0.11406318,\n",
              "          0.33782216,  2.15523537],\n",
              "        [-0.53185892,  0.        ,  0.        , ..., -0.14094454,\n",
              "          0.44296553,  1.95861758],\n",
              "        ...,\n",
              "        [-0.19524659,  0.        ,  0.        , ..., -0.32697834,\n",
              "          0.19267922,  1.87890864],\n",
              "        [-0.49629982,  0.        ,  0.        , ...,  0.05426408,\n",
              "          0.09499987,  2.09966996],\n",
              "        [-0.3020881 ,  0.        ,  0.        , ...,  0.35069668,\n",
              "          0.57690008,  2.03992269]]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "hydra_plus.embed_landmarks(dim, 'mpirun', nrepeat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SXOw1PGLt2sK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbc6ecd-c042-49fe-e57c-db3c249ec220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hyperbolic dimension for non-landmark embedding is 100.\n",
            "Number of batches of nonlandmarks is 4\n",
            "Total nonlandmark time is 0.35960 seconds\n",
            "Non-landmark error:  25.559224697473148\n",
            "Total nonland time:  6.234113693237305\n"
          ]
        }
      ],
      "source": [
        "hydra_plus.embed_nonlandmarks(nprocs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1TsiZwLHuwiR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "21f851b0-7b0b-4666-de51-d3c4b23740f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "begin validation tests...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' -n 250 -L 101 -d 100 -p 4 -s1 /content/L-hydra/data/amazon/ -s2 /content/L-hydra/data/amazon/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "'''\n",
        "\t\tvalidate and save validation meta data in savedir\n",
        "'''\n",
        "import lm_emb_hydra_functions as lm_emb_func\n",
        "\n",
        "self = hydra_plus\n",
        "nprocs3 = nprocs\n",
        "MD = self.metadata\n",
        "nodes = MD['nnodes']\n",
        "landmarks = MD['nL']\n",
        "datadir = MD['datadir']\n",
        "dim = dim\n",
        "# run validation test (must create /results directory)\n",
        "print(\"begin validation tests...\")\n",
        "val_input = lm_emb_func.validation_preproc(nodes, landmarks, dim, nprocs3, datadir, savedir)\n",
        "val_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "j7_6M9S4zyU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f17c9368-0fac-496f-f666-3777ab7375ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating random nonlandmarks...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[140, 142],\n",
              "       [214, 242],\n",
              "       [232, 209],\n",
              "       [186, 165],\n",
              "       [136, 125],\n",
              "       [199, 233],\n",
              "       [191, 231],\n",
              "       [197, 118],\n",
              "       [102, 170],\n",
              "       [243, 170],\n",
              "       [119, 143],\n",
              "       [245, 125],\n",
              "       [185, 207],\n",
              "       [154, 230],\n",
              "       [127, 182],\n",
              "       [205, 127],\n",
              "       [137, 176],\n",
              "       [231, 244],\n",
              "       [145, 182],\n",
              "       [142, 114],\n",
              "       [236, 131],\n",
              "       [211, 233],\n",
              "       [178, 240],\n",
              "       [234, 131],\n",
              "       [231, 180],\n",
              "       [156, 156],\n",
              "       [216, 101],\n",
              "       [176, 173],\n",
              "       [134, 194],\n",
              "       [122, 204],\n",
              "       [184, 177],\n",
              "       [184, 172],\n",
              "       [147, 170],\n",
              "       [217, 206],\n",
              "       [219, 108],\n",
              "       [176, 241],\n",
              "       [202, 218],\n",
              "       [237, 165],\n",
              "       [175, 151],\n",
              "       [181, 129]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "print(\"Generating random nonlandmarks...\")\n",
        "nonland_ids = np.array([i for i in range(nlandmarks,nNodes)])\n",
        "n_test_points = nvalidation\n",
        "n_nonland = len(nonland_ids.copy())\n",
        "n_random = min(n_test_points, n_nonland) # choose at most 20k points\n",
        "ri = rn.choice(n_nonland, n_random)\n",
        "rj = rn.choice(n_nonland, n_random)\n",
        "#print(ri)\n",
        "#print(rj)\n",
        "R = np.array([nonland_ids[ri],nonland_ids[rj]]).T\n",
        "R = vstack([tuple(row) for row in R]).astype(int)\n",
        "R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sdXqajCr1d67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ed55d3-45a3-455d-b19e-b1f2db707c04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.8292328371233917,\n",
              " 1.4015846969131103,\n",
              " 1.6534901612208581,\n",
              " 1.7548159820689468,\n",
              " 1.77341868020228,\n",
              " 1.6300610304231733,\n",
              " 1.618881758937033,\n",
              " 1.6455509036588145,\n",
              " 1.3993872461457488,\n",
              " 1.4102936410635047,\n",
              " 1.3988731152846323,\n",
              " 1.5998263051622068,\n",
              " 1.955655558483036,\n",
              " 1.5097597920245343,\n",
              " 1.527873730484221,\n",
              " 1.5881467024403797,\n",
              " 1.8053578116845717,\n",
              " 1.7644239340642984,\n",
              " 1.2678971716270333,\n",
              " 1.7590500818742156,\n",
              " 1.430601536621047,\n",
              " 1.727158868817884,\n",
              " 1.4905227931607898,\n",
              " 1.409203233193053,\n",
              " 1.6759004047127282,\n",
              " 0.0,\n",
              " 1.497192623976104,\n",
              " 1.7876678519813805,\n",
              " 1.5054030935649665,\n",
              " 1.6636421885963764,\n",
              " 1.5025695952346758,\n",
              " 1.4107357033698775,\n",
              " 1.4344667091981518,\n",
              " 1.6363727902668197,\n",
              " 1.5249874649724795,\n",
              " 1.916655094770981,\n",
              " 1.843816838794322,\n",
              " 1.473318013883497,\n",
              " 1.5739515504471862,\n",
              " 1.6042401541256617]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "Rdist = []\n",
        "for edge in R:\n",
        "    node1 = edge[0]\n",
        "    node2 = edge[1]\n",
        "    Rdist.append(D[node1,node2])\n",
        "Rdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wMddF36I5oRW"
      },
      "outputs": [],
      "source": [
        "savetxt(os.path.join(savedir, 'test_points.txt'),R,fmt='%i')\n",
        "savetxt(os.path.join(savedir, 'test_point_distances.txt'),Rdist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QwQb7xnLIYeF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1fb8dcad-2009-4dd5-e25a-76ac1ba394ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' -n 250 -L 101 -d 100 -p 4 -s1 /content/L-hydra/data/amazon/ -s2 /content/L-hydra/data/amazon/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "val_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "t_aktffRvsKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5744397d-4654-475b-d3d2-9d4a20e761d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ************************************************************\n",
            "Embedding for n = 250 and L = 100 in Dimension 2\n",
            "************************************************************\n",
            "Loading data...\n",
            "\n",
            "Validating...\n",
            "  Validation error:            1.0\n",
            "  Validation error w/ round.:  1.0\n",
            "  Total time for validtion is  0.04361104965209961\n",
            "----------------------------------------\n",
            "Cleaning up data folder...\n",
            "rm: cannot remove '/content/L-hydra/data/amazon/curv.txt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "[\" -n \",\" -L \",\" -d \",\" -p \", \" -s1 \", \" -s2 \"]\n",
        "these flags correspond to = [nodes, landmarks, dim, nproc, dataloc, savedir]\n",
        "change these if you change the values at the top of this notebook\n",
        "'''\n",
        "\n",
        "!python3 lm_emb_validation.py -n 250 -L 100 -d 2 -p 4 -s1 /content/L-hydra/data/amazon/ -s2 /content/L-hydra/data/amazon/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}